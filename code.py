import tensorflow as tf

from tensorflow.contrib import rnn

vocab=[1,2,3,4,5,6,7,8,9,10,11,12,13,14]

import numpy as np

embedding_dim = 110

input_x = tf.placeholder(tf.int32,shape=[None,None])

output_y = tf.placeholder(tf.int32,shape=[None,])

word_embedding = tf.get_variable('embed',shape=[len(vocab),110],dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))

lookup = tf.nn.embedding_lookup(word_embedding,input_x)

without_zero=tf.count_nonzero(input_x,axis=-1)

with tf.variable_scope('encoder') as scope:
    cell=rnn.LSTMCell(num_units=109)
    model=tf.nn.bidirectional_dynamic_rnn(cell,cell,inputs=lookup,sequence_length=without_zero,dtype=tf.float32)


output,(_states_c,states_f)=model

transpo1= tf.transpose(output[0],[1,0,2])
transpo2= tf.transpose(output[1],[1,0,2])

concat = tf.concat((transpo1[-1],transpo2[-1]),axis=-1)

concat2=tf.concat((_states_c.c,states_f.c),axis=-1)

weights_x= tf.get_variable('weight',shape=[2*109,len(vocab)],dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))

bias_x = tf.get_variable('bias',shape=[len(vocab)],dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))

result_matmul = tf.matmul ( concat , weights_x) + bias_x

normalizaton = tf.nn.softmax(result_matmul)

max_prob = tf.argmax(normalizaton,axis=-1)

#cross entropy

ce= tf.nn.sparse_softmax_cross_entropy_with_logits(logits=result_matmul,labels=output_y)
loss = tf.reduce_mean(ce)

#accuracy

acc = tf.reduce_mean(tf.cast((tf.equal(tf.cast(max_prob,tf.int32),output_y)),tf.float32))

train = tf.train.AdamOptimizer().minimize(loss)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    mode_run,emd_run,trans1,trans2,conca_run,conca2_run,result_ma_run,max_pro,accc,sdf=sess.run([output,lookup,transpo1,transpo2,concat,concat2,result_matmul,max_prob,acc,_states_c],feed_dict={input_x:np.random.randint(0,10,[23,6]),output_y:np.random.randint(0,14,[23])})
    print("embedding_output",emd_run.shape)  # 23 x 6 x 110

    print("each_lstm_cell_output",sdf.c.shape)     # 23 x 109

    print("combine_rnn_output",mode_run[0].shape)  #23 x 6 x 109

    print("transpose1",trans1.shape)   #6 x 23 x 109
    print("transpose2",trans2.shape)   #6 x 23 x 109

    print("concat_output_layes",conca_run.shape) #23 x 218

    print("states concat_output",conca2_run.shape) #23 x 218

    print("result_of_matmul",result_ma_run.shape)  #23 x 14

    print("max",max_pro)




#embedding_output (23, 6, 110)
# each_lstm_cell_output (23, 109)
# combine_rnn_output (23, 6, 109)
# transpose1 (6, 23, 109)
# transpose2 (6, 23, 109)
# concat_output_layes (23, 218)
# states concat_output (23, 218)
# result_of_matmul (23, 14)
# max [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]

#probability distribution len of 14 vector for each sequence vector 


# without_taking,max_prob [[0.07097801 0.07147422 0.07103088 0.07138096 0.07195743 0.07219341
#   0.07138678 0.07138267 0.0712225  0.0716541  0.07168832 0.07138323
#   0.07121081 0.07105663]
#  [0.07097419 0.07148512 0.07102346 0.07137904 0.07197029 0.07218575
#   0.07137944 0.07136715 0.07123045 0.07164636 0.07169922 0.07138146
#   0.07121808 0.07106002]
#  [0.07098249 0.07147782 0.07101462 0.0713608  0.07197491 0.07218235
#   0.0713876  0.07136268 0.07122684 0.07162987 0.07170435 0.07139181
#   0.07122517 0.07107872]
#  [0.07097419 0.07148512 0.07102346 0.07137904 0.07197029 0.07218575
#   0.07137944 0.07136715 0.07123045 0.07164636 0.07169922 0.07138146
#   0.07121808 0.07106002]
#  [0.07097568 0.07148066 0.07101669 0.07138454 0.07196721 0.07217896
#   0.07138873 0.0713719  0.07123213 0.07163537 0.07168526 0.07137992
#   0.07123061 0.07107234]
#  [0.07097419 0.07148512 0.07102346 0.07137904 0.07197029 0.07218575
#   0.07137944 0.07136715 0.07123045 0.07164636 0.07169922 0.07138146
#   0.07121808 0.07106002]
#  [0.0709731  0.07147659 0.07101922 0.07137746 0.0719696  0.07218345
#   0.07138161 0.07137027 0.07122559 0.07164469 0.07169428 0.07138652
#   0.07122072 0.07107683]
#  [0.07096908 0.07146581 0.07103316 0.07137336 0.07197709 0.07218733
#   0.07138272 0.07135194 0.07123198 0.07164004 0.07169534 0.07139447
#   0.071224   0.07107372]
#  [0.07097419 0.07148512 0.07102346 0.07137904 0.07197029 0.07218575
#   0.07137944 0.07136715 0.07123045 0.07164636 0.07169922 0.07138146
#   0.07121808 0.07106002]
#  [0.07097419 0.07148512 0.07102346 0.07137904 0.07197029 0.07218575
#   0.07137944 0.07136715 0.07123045 0.07164636 0.07169922 0.07138146
#   0.07121808 0.07106002]
#  [0.07097405 0.07147524 0.07102135 0.0713769  0.07196625 0.07217744
#   0.07138281 0.07136552 0.07123732 0.07163347 0.07170411 0.07138985
#   0.07122552 0.07107013]
#  [0.07097419 0.07148512 0.07102346 0.07137904 0.07197029 0.07218575
#   0.07137944 0.07136715 0.07123045 0.07164636 0.07169922 0.07138146
#   0.07121808 0.07106002]
#  [0.07097419 0.07148512 0.07102346 0.07137904 0.07197029 0.07218575
#   0.07137944 0.07136715 0.07123045 0.07164636 0.07169922 0.07138146
#   0.07121808 0.07106002]
#  [0.07097612 0.07146807 0.07101972 0.07137397 0.07197471 0.0721902
#   0.07138067 0.07137077 0.07123736 0.07163092 0.07170945 0.07138563
#   0.07121315 0.07106921]
#  [0.07097477 0.07147539 0.07102212 0.07137576 0.07196814 0.07218321
#   0.07138339 0.07136988 0.07122897 0.07163927 0.07170686 0.07138506
#   0.07121938 0.07106773]
#  [0.07097419 0.07148512 0.07102346 0.07137904 0.07197029 0.07218575
#   0.07137944 0.07136715 0.07123045 0.07164636 0.07169922 0.07138146
#   0.07121808 0.07106002]
#  [0.07098284 0.07147032 0.07103031 0.0713651  0.07197215 0.07219481
#   0.07138121 0.07136188 0.07123093 0.07163452 0.07170828 0.07138411
#   0.07121652 0.07106697]
#  [0.07097419 0.07148512 0.07102346 0.07137904 0.07197029 0.07218575
#   0.07137944 0.07136715 0.07123045 0.07164636 0.07169922 0.07138146
#   0.07121808 0.07106002]
#  [0.07099048 0.07148419 0.07101794 0.07136719 0.07197249 0.07218304
#   0.07138808 0.07136009 0.07121219 0.07163945 0.0716904  0.07139073
#   0.0712239  0.07107974]
#  [0.07097419 0.07148512 0.07102346 0.07137904 0.07197029 0.07218575
#   0.07137944 0.07136715 0.07123045 0.07164636 0.07169922 0.07138146
#   0.07121808 0.07106002]
#  [0.07097419 0.07148512 0.07102346 0.07137904 0.07197029 0.07218575
#   0.07137944 0.07136715 0.07123045 0.07164636 0.07169922 0.07138146
#   0.07121808 0.07106002]
#  [0.07097419 0.07148512 0.07102346 0.07137904 0.07197029 0.07218575
#   0.07137944 0.07136715 0.07123045 0.07164636 0.07169922 0.07138146
#   0.07121808 0.07106002]
#  [0.07097419 0.07148512 0.07102346 0.07137904 0.07197029 0.07218575
#   0.07137944 0.07136715 0.07123045 0.07164636 0.07169922 0.07138146
#   0.07121808 0.07106002]]
